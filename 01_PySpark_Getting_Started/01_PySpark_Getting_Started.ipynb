{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics Covered:\n",
    "    1. Creating Spark SQL Session\n",
    "    2. Creating Spark Context\n",
    "    3. Using RDD, map and take\n",
    "    4. Reading text file and applying actions\n",
    "    5. Examples: Transformations\n",
    "    6. Connecting to MySQL\n",
    "    7. Read and Write: CSV\n",
    "    8. Read and Write: Parquet\n",
    "    9. SQL on dataframes.\n",
    "    10. DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Creating a Spark SQL Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Creating a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x14ea545d748>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('spark-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"spark-basic\")\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Using RDD, map and take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 0), (3, 1), (4, 0), (5, 1), (6, 0), (7, 1), (8, 0), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1000)).map(lambda x:(x,x%2)).take(10)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Reading text file and applying actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading TXT File and use pre-defined functions:\n",
    "count, collect,first,take,takeSample,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a text file:\n",
    "txt = sc.textFile('hello.txt')\n",
    "type(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first():  Line 1\n",
      "count():  1244\n",
      "take():  ['Line 1', 'Line 2', 'Line 3', 'Line 4', 'Line 5']\n",
      "takeSample():  ['Last Line it is.', 'This is line number two', 'Welcome to PySpark!!', 'Last Line it is.', 'Line 3']\n"
     ]
    }
   ],
   "source": [
    "txt.collect() # rdd got converted to list format.\n",
    "print(\"first(): \",txt.first())\n",
    "print(\"count(): \",txt.count())\n",
    "print(\"take(): \",txt.take(5))\n",
    "print(\"takeSample(): \",txt.takeSample(False,5,1)) # takes 5 samples with or without(True or False) replacement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Examples: Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformations: map,flastmap,filter,mapPartitions,mapPartitionsWithIndex,sample,union,intersection,distinct groupBy,keyBy,Zip,zipwithIndex,Coalesce,Repartition,sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Red2', 'Green2', 'Blue2', 'Yellow2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using map:\n",
    "x = sc.parallelize([\"Red\",\"Green\",\"Blue\",\"Yellow\"])\n",
    "y = x.map(lambda x:(x+str(2)))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#Using flatMap:\n",
    "x = sc.parallelize([2,3,4])\n",
    "y = x.flatMap(lambda x:range(1,x)).collect()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 8, 8, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Filter:\n",
    "x = sc.parallelize([2,5,6,8,1,5,8,9,6,3])\n",
    "y = x.filter(lambda x : x%2==0).collect()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 6, 8]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#Take one sample without replacement from each cluster:\n",
    "x = sc.parallelize(range(1,10))\n",
    "print(x.sample(False,0.8,2).collect()) # (with or without replacement,fraction,seed)\n",
    "print(x.sample(False,1,2).collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union:  [1, 2, 3, 4, 5, 6, 7, 8, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  Intersection:  [6, 8, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "#Union&intersection:\n",
    "x = sc.parallelize(range(1,9))\n",
    "y = sc.parallelize(range(5,15))\n",
    "z = x.union(y).collect()\n",
    "z2 = x.intersection(y).collect()\n",
    "print(\"Union: \",z,\" Intersection: \",z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('D', 10), ('F', 222), ('I', 15), ('Z', 28)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortBy:\n",
    "#x = sc.parallelize([2,8,9,5,4,7,1,1,2,3])\n",
    "#x.sortBy(lambda x:x,True).collect()\n",
    "y = sc.parallelize([(\"F\",222),(\"Z\",28),(\"I\",15),(\"D\",10)])\n",
    "y.sortBy(lambda x:x,True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 15]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapPartitios:\n",
    "x = sc.parallelize([1,2,3,4,5,6],2)\n",
    "def f(cluster): yield sum(cluster)\n",
    "x.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapPartitios with index:\n",
    "x = sc.parallelize([1,2,3,4,5,6],5)\n",
    "def f(index,cluster): yield index\n",
    "x.mapPartitionsWithIndex(f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy:\n",
    "rdd = sc.parallelize([1,1,2,3,5,8])\n",
    "result = rdd.groupBy(lambda x: x%2).collect()\n",
    "sorted([(x,sorted(y)) for (x,y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [[0], [0]]),\n",
       " (1, [[1], [1]]),\n",
       " (2, [[], [2]]),\n",
       " (3, [[], [3]]),\n",
       " (4, [[2], [4]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keyBy:\n",
    "x = sc.parallelize(range(0,3)).keyBy(lambda x:x*x)\n",
    "y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
    "[(x, list(map(list,y))) for x,y in sorted(x.cogroup(y).collect())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip:\n",
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000,1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('b', 1), ('c', 2), ('d', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip:\n",
    "sc.parallelize([\"a\",\"b\",\"c\",\"d\"]).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5], [6, 7]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partitioning:\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7],4)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 5, 6, 7], [2, 3]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coalesce:\n",
    "sc.parallelize([1,2,3,4,5],3).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3, 4, 5]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5],3).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Connecting to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"sonuresodb\"\n",
    "table = \"lyrics\"\n",
    "user = \"root\"\n",
    "password  = \"\"\n",
    "\n",
    "jdbcDF = spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlserver://localhost:3306;databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password).option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Read and Write: CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reading csv with SQL spark: spark.read.csv(filename)\n",
    "    1. Reading one csv file:                  spark.read.csv(\"file1.csv\",header=True)\n",
    "    2. Reading multiple csv file:             spark.read.csv(\"file1.csv,file2.csv,file3.csv\",header=True)\n",
    "    3. Reading all csv file in a directory:   spark.read.csv(\"folder path\",header=True)\n",
    "    4. options: a) ('delimiter'=',') or ('sep'=',')\n",
    "                b) (\"inferSchema\",True)\n",
    "                c) (\"header\",True)\n",
    "                d) New Schema: spark.read.csv(\"file.csv\",schema=new_schema)\n",
    "2. Writing dataframes as csv:  df.write.csv(foldername)\n",
    "    1. options: a) header=True :        df.write.csv(\"foldernm/\",header=True)\n",
    "                b) delimiter='|' :      df.write.options(delimiter='|',header=True).csv(\"newmovies/\")\n",
    "    2. mode:    a) overwrite :          df.write.mode('overwrite').csv(\"newmovies/\")\n",
    "                b) append      c) ignore       d) error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1.1 to read a csv:\n",
    "df=spark.read.option(\"header\",True).option(\"delimiter\",\",\").option(\"inferSchema\",True).csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1.2 (short method) to read a csv:\n",
    "df=spark.read.csv(\"movies.csv\",header=True,sep=\",\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_2.1 to read a csv with new_schema:\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType\n",
    "new_schema = StructType() \\\n",
    "      .add(\"year\",IntegerType(),True) \\\n",
    "      .add(\"imdb\",StringType(),True) \\\n",
    "      .add(\"title\",StringType(),True) \\\n",
    "      .add(\"test\",StringType(),True) \\\n",
    "      .add(\"clean_test\",StringType(),True) \\\n",
    "      .add(\"binary\",StringType(),True) \\\n",
    "      .add(\"budget\",DoubleType(),True) \\\n",
    "      .add(\"domgross\",StringType(),True) \\\n",
    "      .add(\"intgross\",IntegerType(),True) \\\n",
    "      .add(\"code\",StringType(),True) \\\n",
    "      .add(\"budget_2013$\",DoubleType(),True) \\\n",
    "      .add(\"domgross_2013$\",StringType(),True) \\\n",
    "      .add(\"intgross_2013$\",IntegerType(),True) \\\n",
    "      .add(\"period code\",IntegerType(),True) \\\n",
    "      .add(\"decade code\",IntegerType(),True)\n",
    "\n",
    "df=spark.read.csv(\"movies.csv\",header=True,schema=new_schema)\n",
    "df = df.toDF(*[col.replace(' ','_').replace(':','').replace('-','_').replace('$','') for col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_2.2 to read a csv with new_schema(better way):\n",
    "def get_schema(d):\n",
    "    new_schema = StructType()\n",
    "    for key, value in d.items():\n",
    "        new_schema.add(key,value,True)\n",
    "    return new_schema\n",
    "\n",
    "d = {'year':IntegerType(),'imdb':StringType(),'title':StringType(),'test':StringType(),'clean_test':StringType(),\n",
    "    'binary':StringType(),'budget':DoubleType(),'domgross':StringType(),'intgross':StringType(),'code':StringType(),\n",
    "    'budget_2013$':DoubleType(),'domgross_2013$':StringType(),'intgross_2013$':StringType(),'period code':IntegerType(),\n",
    "    'decade code':IntegerType()}\n",
    "df=spark.read.csv(\"movies.csv\",header=True,schema=get_schema(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1 to write: create a folder.\n",
    "df.write.options(delimiter='|',header=True).mode('overwrite').csv(\"newmovies/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Read and Write: Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Writing Parquets:\n",
    "    1. method_1 : df.write.parquet(\"filename.parquet\")\n",
    "    2. method_2 : df.write.mode('overwrite').parquet(\"filename.parquet\")\n",
    "        a) mode:  overwrite, append, error, ignore\n",
    "    3. method_3 : df.write.partitionBy(\"col1\",\"col2\").parquet(\"filename.parquet\")\n",
    "    4. \n",
    "2. Reading parquets:\n",
    "    1. method_1 :  spark.read.parquet(\"filename.parquet\")\n",
    "    2. method_2 :  Partitioned Parquet: spark.read.parquet(\"movies_partitioned.parquet/binary=FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "# removing special char from col_names:\n",
    "df = df.toDF(*[col.replace(' ','_').replace(':','').replace('-','_').replace('$','') for col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1: write parquet:\n",
    "df.write.parquet(\"movies_\"+str(int(dt.timestamp(dt.now())))+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_2: write parquet with overwrite mode:\n",
    "df.write.mode('overwrite').parquet(\"movies.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_3: write parquet with partition:\n",
    "df.write.partitionBy(\"binary\").parquet(\"movies_partitioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_1: Read parquet file:\n",
    "par_df = spark.read.parquet(\"movies.parquet\")\n",
    "len(par_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_2: Read partitioned parquet file:\n",
    "par_df = spark.read.parquet(\"movies_partitioned.parquet/binary=FAIL\")\n",
    "len(par_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. SQL on dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|binary|count|\n",
      "+------+-----+\n",
      "|  FAIL|  991|\n",
      "|  PASS|  803|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df_table\")\n",
    "spark.sql(\"select binary, count(binary) as count from df_table group by binary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "1. df.describe()\n",
    "2. df.distinct()\n",
    "3. df.subtract(df)\n",
    "4. df.crosstab(col1,col2)\n",
    "5. df.dropDuplicates()\n",
    "6. df.dropna()\n",
    "7. df.fillna(value)\n",
    "8. df.filter(df.col==value)\n",
    "9. df.groupby(col).agg({'col':'mean'})\n",
    "10. df.sample(False,fraction,seed)\n",
    "11. df.orderBy(df.col.desc)  # desc, asc,. OR df.sort(col) \n",
    "12. Adding a new column: df.withColumn('id',df.prev_id)\n",
    "13. df.drop(col)\n",
    "14. User defined function: udf(lambda x: x+1)\n",
    "15. Renaming Column names:\n",
    "16. data type casting\n",
    "17. Using Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+\n",
      "|summary|              budget|        budget_2013|       period_code|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "|  count|                1794|               1794|              1615|\n",
      "|   mean|4.4826462614269786E7|5.546460845150502E7| 2.419814241486068|\n",
      "| stddev| 4.818602611895356E7|5.491863559804196E7|1.1946197915091876|\n",
      "|    min|                7000|               8632|                 1|\n",
      "|    max|           425000000|          461435929|                 5|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "1# describe: overall info of the col:\n",
    "df.select(['budget','budget_2013','period_code']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "+------+\n",
      "|binary|\n",
      "+------+\n",
      "|  FAIL|\n",
      "|  PASS|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "2# distinct:\n",
    "print(df.select(['year']).distinct().count())\n",
    "df.select(['binary']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subtract of fails:  3\n",
      "Wall time: 2.29 s\n",
      "Directly counting pass:  3\n",
      "Wall time: 262 ms\n"
     ]
    }
   ],
   "source": [
    "3# using subtract: count of PASS in year 1982:\n",
    "%time print(\"Using subtract of fails: \",df[df.year=='1982'].subtract(df[df.binary=='FAIL']).count())\n",
    "# Same can be done efficiently as:\n",
    "%time print(\"Directly counting pass: \",df[df.year=='1982'][df.binary=='PASS'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+\n",
      "|year_binary|FAIL|PASS|\n",
      "+-----------+----+----+\n",
      "|       1995|  18|  18|\n",
      "|       1982|  11|   3|\n",
      "|       1971|   5|   0|\n",
      "|       1999|  33|  23|\n",
      "|       1988|  10|   9|\n",
      "+-----------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "4# We can use crosstab operation: Every year, how many PASS, and FAIL.\n",
    "# Only two columns are allowed.\n",
    "df.crosstab('year','binary').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5# Dropping Duplicates Values.\n",
    "df.select('year').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------------+-------+----------+------+--------+--------+--------+--------+-----------+-------------+-------------+-----------+-----------+\n",
      "|year|     imdb|          title|   test|clean_test|binary|  budget|domgross|intgross|    code|budget_2013|domgross_2013|intgross_2013|period_code|decade_code|\n",
      "+----+---------+---------------+-------+----------+------+--------+--------+--------+--------+-----------+-------------+-------------+-----------+-----------+\n",
      "|1984|tt0088161|         Splash| notalk|    notalk|  FAIL| 8000000|62599495|62599495|1984FAIL|   17931552|    140313263|    140313263|       null|       null|\n",
      "|1986|tt0091225|Howard The Duck|dubious|   dubious|  FAIL|30000000|16295774|16295774|1986FAIL|   63712942|     34608390|     34608390|       null|       null|\n",
      "+----+---------+---------------+-------+----------+------+--------+--------+--------+--------+-----------+-------------+-------------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "6# Dropna & fillna:\n",
    "# dropna return df wihtout null value :\n",
    "df.subtract(df.dropna()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------------+-------+----------+------+--------+--------+--------+--------+-----------+-------------+-------------+-----------+-----------+\n",
      "|year|     imdb|          title|   test|clean_test|binary|  budget|domgross|intgross|    code|budget_2013|domgross_2013|intgross_2013|period_code|decade_code|\n",
      "+----+---------+---------------+-------+----------+------+--------+--------+--------+--------+-----------+-------------+-------------+-----------+-----------+\n",
      "|1984|tt0088161|         Splash| notalk|    notalk|  FAIL| 8000000|62599495|62599495|1984FAIL|   17931552|    140313263|    140313263|        333|        333|\n",
      "|1986|tt0091225|Howard The Duck|dubious|   dubious|  FAIL|30000000|16295774|16295774|1986FAIL|   63712942|     34608390|     34608390|        333|        333|\n",
      "+----+---------+---------------+-------+----------+------+--------+--------+--------+--------+-----------+-------------+-------------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "7#fillna: return the all data after filling passed value:\n",
    "# df.fillna(value)\n",
    "df.subtract(df.dropna()).fillna(333).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8# Using filter: getting count of PASS.\n",
    "df.filter(df.binary=='PASS').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|binary|count|\n",
      "+------+-----+\n",
      "|  FAIL|  991|\n",
      "|  PASS|  803|\n",
      "+------+-----+\n",
      "\n",
      "+------+-------------+-------------+-------------------+\n",
      "|binary|count(binary)|max(intgross)|        avg(budget)|\n",
      "+------+-------------+-------------+-------------------+\n",
      "|  FAIL|          991|     99965792|5.041528926639758E7|\n",
      "|  PASS|          803|     99982632|3.792916845205479E7|\n",
      "+------+-------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "9# Group By:\n",
    "df.groupby('binary').count().show()\n",
    "df.groupby('binary').agg({'binary':'count','budget':'mean','intgross':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------------+----------------+----------+------+------+---------+---------+\n",
      "|year|     imdb|               title|            test|clean_test|binary|budget| domgross| intgross|\n",
      "+----+---------+--------------------+----------------+----------+------+------+---------+---------+\n",
      "|2013|tt1711425|       21 &amp; Over|          notalk|    notalk|  FAIL| 1.3E7| 25682380| 42195766|\n",
      "|2012|tt1343727|            Dredd 3D|     ok-disagree|        ok|  PASS| 4.5E7| 13414714| 40868994|\n",
      "|2013|tt1985966|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL| 7.8E7|119640264|271725448|\n",
      "+----+---------+--------------------+----------------+----------+------+------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10# Creating Sample Dataframe:\n",
    "# df.sample(False,0.4,11): (withOrWithoutReplacement,fraction,seed)\n",
    "df1 = df.select(df.columns[:-6]).sample(False,0.2,12)\n",
    "df1.show(3)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------------+------+----------+------+------+--------+--------+\n",
      "|year|     imdb|               title|  test|clean_test|binary|budget|domgross|intgross|\n",
      "+----+---------+--------------------+------+----------+------+------+--------+--------+\n",
      "|1994|tt0109445|             Clerks.|notalk|    notalk|  FAIL| 27000| 3073428| 3894240|\n",
      "|1974|tt0072308|The Towering Inferno|notalk|    notalk|  FAIL|140000|26572439|26572439|\n",
      "|2008|tt1152850|      Wendy and Lucy|    ok|        ok|  PASS|300000|  865695| 1416046|\n",
      "|1985|tt0091578|My Beautiful Laun...|   men|       men|  FAIL|400000|    #N/A|    #N/A|\n",
      "|1975|tt0071853|Monty Python and ...|   men|       men|  FAIL|400000| 3427696| 5028948|\n",
      "+----+---------+--------------------+------+----------+------+------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+---------+--------------------+------+----------+------+--------+---------+---------+\n",
      "|year|     imdb|               title|  test|clean_test|binary|  budget| domgross| intgross|\n",
      "+----+---------+--------------------+------+----------+------+--------+---------+---------+\n",
      "|1971|tt0067065|Escape from the P...|notalk|    notalk|  FAIL| 2500000| 12300000| 12300000|\n",
      "|1973|tt0069704|   American Graffiti|   men|       men|  FAIL|  777000|115000000|140000000|\n",
      "|1974|tt0071360|    The Conversation|notalk|    notalk|  FAIL| 1600000|  4420000|  4420000|\n",
      "|1974|tt0072308|The Towering Inferno|notalk|    notalk|  FAIL|  140000| 26572439| 26572439|\n",
      "|1975|tt0073195|                Jaws|notalk|    notalk|  FAIL|12000000|260000000|470700000|\n",
      "+----+---------+--------------------+------+----------+------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "11# Using df.orderBy\n",
    "df1.orderBy(df1.budget.asc()).show(5)\n",
    "#df1.orderBy(df1.budget.desc()).show(5)\n",
    "df1.sort(\"year\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+---------+\n",
      "|year|     imdb|               title|            test|clean_test|binary|  budget| domgross| intgross|       id|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+---------+\n",
      "|2013|tt1711425|       21 &amp; Over|          notalk|    notalk|  FAIL|13000000| 25682380| 42195766|tt1711425|\n",
      "|2012|tt1343727|            Dredd 3D|     ok-disagree|        ok|  PASS|45000000| 13414714| 40868994|tt1343727|\n",
      "|2013|tt1985966|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL|78000000|119640264|271725448|tt1985966|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "12# adding a new Column:\n",
    "df1.withColumn('id',df1.imdb).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|year|               title|            test|clean_test|binary|  budget| domgross| intgross|\n",
      "+----+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|2013|       21 &amp; Over|          notalk|    notalk|  FAIL|13000000| 25682380| 42195766|\n",
      "|2012|            Dredd 3D|     ok-disagree|        ok|  PASS|45000000| 13414714| 40868994|\n",
      "|2013|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL|78000000|119640264|271725448|\n",
      "+----+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+--------------------+----------+------+--------+---------+---------+\n",
      "|year|               title|clean_test|binary|  budget| domgross| intgross|\n",
      "+----+--------------------+----------+------+--------+---------+---------+\n",
      "|2013|       21 &amp; Over|    notalk|  FAIL|13000000| 25682380| 42195766|\n",
      "|2012|            Dredd 3D|        ok|  PASS|45000000| 13414714| 40868994|\n",
      "|2013|Cloudy with a Cha...|   nowomen|  FAIL|78000000|119640264|271725448|\n",
      "+----+--------------------+----------+------+--------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "13# droping a or more columns:\n",
    "df1.drop('imdb').show(3)\n",
    "df1.drop('imdb','test').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+-------+\n",
      "|year|     imdb|               title|            test|clean_test|binary|  budget| domgross| intgross|year_is|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+-------+\n",
      "|2013|tt1711425|       21 &amp; Over|          notalk|    notalk|  FAIL|13000000| 25682380| 42195766|    odd|\n",
      "|2012|tt1343727|            Dredd 3D|     ok-disagree|        ok|  PASS|45000000| 13414714| 40868994|   even|\n",
      "|2013|tt1985966|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL|78000000|119640264|271725448|    odd|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "14# Using user defined function: \n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "F1 = udf(lambda x: 'even' if x%2==0 else 'odd', StringType())\n",
    "df1.withColumn(\"year_is\",F1(df['year'])).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|year|  id_imdb|               title|            test|clean_test|binary|  budget| domgross| intgross|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|2013|tt1711425|       21 &amp; Over|          notalk|    notalk|  FAIL|13000000| 25682380| 42195766|\n",
      "|2012|tt1343727|            Dredd 3D|     ok-disagree|        ok|  PASS|45000000| 13414714| 40868994|\n",
      "|2013|tt1985966|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL|78000000|119640264|271725448|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|year| id_imdb2|          movie_name|            test|clean_test|binary|  budget| domgross| intgross|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|2013|tt1711425|       21 &amp; Over|          notalk|    notalk|  FAIL|13000000| 25682380| 42195766|\n",
      "|2012|tt1343727|            Dredd 3D|     ok-disagree|        ok|  PASS|45000000| 13414714| 40868994|\n",
      "|2013|tt1985966|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL|78000000|119640264|271725448|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "15# renaming one column name:\n",
    "df1.withColumnRenamed(\"imdb\",\"id_imdb\").show(3)\n",
    "# renaming all columns names:\n",
    "x = df1.columns\n",
    "x[1:3] = ['id_imdb2','movie_name']\n",
    "df1.toDF(*x).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|  year|     imdb|               title|            test|clean_test|binary|  budget| domgross| intgross|\n",
      "+------+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|2013.0|tt1711425|       21 &amp; Over|          notalk|    notalk|  FAIL|13000000| 25682380| 42195766|\n",
      "|2012.0|tt1343727|            Dredd 3D|     ok-disagree|        ok|  PASS|45000000| 13414714| 40868994|\n",
      "|2013.0|tt1985966|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL|78000000|119640264|271725448|\n",
      "+------+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|year|     imdb|               title|            test|clean_test|binary|  budget| domgross| intgross|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "|2013|tt1711425|       21 &amp; Over|          notalk|    notalk|  FAIL|13000000| 25682380| 42195766|\n",
      "|2012|tt1343727|            Dredd 3D|     ok-disagree|        ok|  PASS|45000000| 13414714| 40868994|\n",
      "|2013|tt1985966|Cloudy with a Cha...|nowomen-disagree|   nowomen|  FAIL|78000000|119640264|271725448|\n",
      "+----+---------+--------------------+----------------+----------+------+--------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#16. Data Type casting:\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# int to double:\n",
    "df1.withColumn('year', F.col('year').cast(DoubleType())).show(3)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+\n",
      "|year| intgross|rank|\n",
      "+----+---------+----+\n",
      "|1990|517600000|   1|\n",
      "|1990|243700000|   2|\n",
      "|1990|  5017971|   3|\n",
      "|1975|470700000|   1|\n",
      "|1975|  5028948|   2|\n",
      "|1977| 38251425|   1|\n",
      "|2003|266685242|   1|\n",
      "+----+---------+----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 17. Using Window function:\n",
    "# 17.1 performaing: year wise ranking of intgross:\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "windowSpec = Window().partitionBy(['year']).orderBy(df1['intgross'].desc())\n",
    "df1.withColumn(\"rank\",F.rank().over(windowSpec)).select('year','intgross','rank').show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
