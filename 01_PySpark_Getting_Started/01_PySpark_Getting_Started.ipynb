{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics Covered:\n",
    "    1. Creating Spark SQL Session\n",
    "    2. Creating Spark Context\n",
    "    3. Using RDD, map and take\n",
    "    4. Reading text file and applying actions\n",
    "    5. Examples: Transformations\n",
    "    6. Connecting to MySQL\n",
    "    7. Read and Write: CSV\n",
    "    8. Read and Write: Parquet\n",
    "    9. SQL on dataframes.\n",
    "    10. DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Creating a Spark SQL Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Creating a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x14ea545d748>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('spark-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"spark-basic\")\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Using RDD, map and take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 0), (3, 1), (4, 0), (5, 1), (6, 0), (7, 1), (8, 0), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1000)).map(lambda x:(x,x%2)).take(10)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Reading text file and applying actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading TXT File and use pre-defined functions:\n",
    "count, collect,first,take,takeSample,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a text file:\n",
    "txt = sc.textFile('hello.txt')\n",
    "type(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first():  Line 1\n",
      "count():  1244\n",
      "take():  ['Line 1', 'Line 2', 'Line 3', 'Line 4', 'Line 5']\n",
      "takeSample():  ['Last Line it is.', 'This is line number two', 'Welcome to PySpark!!', 'Last Line it is.', 'Line 3']\n"
     ]
    }
   ],
   "source": [
    "txt.collect() # rdd got converted to list format.\n",
    "print(\"first(): \",txt.first())\n",
    "print(\"count(): \",txt.count())\n",
    "print(\"take(): \",txt.take(5))\n",
    "print(\"takeSample(): \",txt.takeSample(False,5,1)) # takes 5 samples with or without(True or False) replacement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Examples: Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformations: map,flastmap,filter,mapPartitions,mapPartitionsWithIndex,sample,union,intersection,distinct groupBy,keyBy,Zip,zipwithIndex,Coalesce,Repartition,sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Red2', 'Green2', 'Blue2', 'Yellow2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using map:\n",
    "x = sc.parallelize([\"Red\",\"Green\",\"Blue\",\"Yellow\"])\n",
    "y = x.map(lambda x:(x+str(2)))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#Using flatMap:\n",
    "x = sc.parallelize([2,3,4])\n",
    "y = x.flatMap(lambda x:range(1,x)).collect()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 8, 8, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Filter:\n",
    "x = sc.parallelize([2,5,6,8,1,5,8,9,6,3])\n",
    "y = x.filter(lambda x : x%2==0).collect()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 6, 8]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#Take one sample without replacement from each cluster:\n",
    "x = sc.parallelize(range(1,10))\n",
    "print(x.sample(False,0.8,2).collect()) # (with or without replacement,fraction,seed)\n",
    "print(x.sample(False,1,2).collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union:  [1, 2, 3, 4, 5, 6, 7, 8, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  Intersection:  [6, 8, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "#Union&intersection:\n",
    "x = sc.parallelize(range(1,9))\n",
    "y = sc.parallelize(range(5,15))\n",
    "z = x.union(y).collect()\n",
    "z2 = x.intersection(y).collect()\n",
    "print(\"Union: \",z,\" Intersection: \",z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('D', 10), ('F', 222), ('I', 15), ('Z', 28)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortBy:\n",
    "#x = sc.parallelize([2,8,9,5,4,7,1,1,2,3])\n",
    "#x.sortBy(lambda x:x,True).collect()\n",
    "y = sc.parallelize([(\"F\",222),(\"Z\",28),(\"I\",15),(\"D\",10)])\n",
    "y.sortBy(lambda x:x,True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 15]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapPartitios:\n",
    "x = sc.parallelize([1,2,3,4,5,6],2)\n",
    "def f(cluster): yield sum(cluster)\n",
    "x.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapPartitios with index:\n",
    "x = sc.parallelize([1,2,3,4,5,6],5)\n",
    "def f(index,cluster): yield index\n",
    "x.mapPartitionsWithIndex(f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy:\n",
    "rdd = sc.parallelize([1,1,2,3,5,8])\n",
    "result = rdd.groupBy(lambda x: x%2).collect()\n",
    "sorted([(x,sorted(y)) for (x,y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [[0], [0]]),\n",
       " (1, [[1], [1]]),\n",
       " (2, [[], [2]]),\n",
       " (3, [[], [3]]),\n",
       " (4, [[2], [4]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keyBy:\n",
    "x = sc.parallelize(range(0,3)).keyBy(lambda x:x*x)\n",
    "y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
    "[(x, list(map(list,y))) for x,y in sorted(x.cogroup(y).collect())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip:\n",
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000,1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('b', 1), ('c', 2), ('d', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip:\n",
    "sc.parallelize([\"a\",\"b\",\"c\",\"d\"]).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5], [6, 7]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partitioning:\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7],4)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 5, 6, 7], [2, 3]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coalesce:\n",
    "sc.parallelize([1,2,3,4,5],3).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3, 4, 5]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5],3).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Connecting to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"sonuresodb\"\n",
    "table = \"lyrics\"\n",
    "user = \"root\"\n",
    "password  = \"\"\n",
    "\n",
    "jdbcDF = spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlserver://localhost:3306;databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password).option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Read and Write: CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reading csv with SQL spark: spark.read.csv(filename)\n",
    "    1. Reading one csv file:                  spark.read.csv(\"file1.csv\",header=True)\n",
    "    2. Reading multiple csv file:             spark.read.csv(\"file1.csv,file2.csv,file3.csv\",header=True)\n",
    "    3. Reading all csv file in a directory:   spark.read.csv(\"folder path\",header=True)\n",
    "    4. options: a) ('delimiter'=',') or ('sep'=',')\n",
    "                b) (\"inferSchema\",True)\n",
    "                c) (\"header\",True)\n",
    "                d) New Schema: spark.read.csv(\"file.csv\",schema=new_schema)\n",
    "2. Writing dataframes as csv:  df.write.csv(foldername)\n",
    "    1. options: a) header=True :        df.write.csv(\"foldernm/\",header=True)\n",
    "                b) delimiter='|' :      df.write.options(delimiter='|',header=True).csv(\"newmovies/\")\n",
    "    2. mode:    a) overwrite :          df.write.mode('overwrite').csv(\"newmovies/\")\n",
    "                b) append      c) ignore       d) error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1.1 to read a csv:\n",
    "df=spark.read.option(\"header\",True).option(\"delimiter\",\",\").option(\"inferSchema\",True).csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1.2 (short method) to read a csv:\n",
    "df=spark.read.csv(\"movies.csv\",header=True,sep=\",\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_2.1 to read a csv with new_schema:\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType\n",
    "new_schema = StructType() \\\n",
    "      .add(\"year\",IntegerType(),True) \\\n",
    "      .add(\"imdb\",StringType(),True) \\\n",
    "      .add(\"title\",StringType(),True) \\\n",
    "      .add(\"test\",StringType(),True) \\\n",
    "      .add(\"clean_test\",StringType(),True) \\\n",
    "      .add(\"binary\",StringType(),True) \\\n",
    "      .add(\"budget\",DoubleType(),True) \\\n",
    "      .add(\"domgross\",StringType(),True) \\\n",
    "      .add(\"intgross\",StringType(),True) \\\n",
    "      .add(\"code\",StringType(),True) \\\n",
    "      .add(\"budget_2013$\",DoubleType(),True) \\\n",
    "      .add(\"domgross_2013$\",StringType(),True) \\\n",
    "      .add(\"intgross_2013$\",StringType(),True) \\\n",
    "      .add(\"period code\",IntegerType(),True) \\\n",
    "      .add(\"decade code\",IntegerType(),True)\n",
    "\n",
    "df=spark.read.csv(\"movies.csv\",header=True,schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_2.2 to read a csv with new_schema(better way):\n",
    "def get_schema(d):\n",
    "    new_schema = StructType()\n",
    "    for key, value in d.items():\n",
    "        new_schema.add(key,value,True)\n",
    "    return new_schema\n",
    "\n",
    "d = {'year':IntegerType(),'imdb':StringType(),'title':StringType(),'test':StringType(),'clean_test':StringType(),\n",
    "    'binary':StringType(),'budget':DoubleType(),'domgross':StringType(),'intgross':StringType(),'code':StringType(),\n",
    "    'budget_2013$':DoubleType(),'domgross_2013$':StringType(),'intgross_2013$':StringType(),'period code':IntegerType(),\n",
    "    'decade code':IntegerType()}\n",
    "df=spark.read.csv(\"movies.csv\",header=True,schema=get_schema(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1 to write: create a folder.\n",
    "df.write.options(delimiter='|',header=True).mode('overwrite').csv(\"newmovies/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Read and Write: Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Writing Parquets:\n",
    "    1. method_1 : df.write.parquet(\"filename.parquet\")\n",
    "    2. method_2 : df.write.mode('overwrite').parquet(\"filename.parquet\")\n",
    "        a) mode:  overwrite, append, error, ignore\n",
    "    3. method_3 : df.write.partitionBy(\"col1\",\"col2\").parquet(\"filename.parquet\")\n",
    "    4. \n",
    "2. Reading parquets:\n",
    "    1. method_1 :  spark.read.parquet(\"filename.parquet\")\n",
    "    2. method_2 :  Partitioned Parquet: spark.read.parquet(\"movies_partitioned.parquet/binary=FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "# removing special char from col_names:\n",
    "df = df.toDF(*[col.replace(' ','_').replace(':','').replace('-','_').replace('$','') for col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_1: write parquet:\n",
    "df.write.parquet(\"movies_\"+str(int(dt.timestamp(dt.now())))+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_2: write parquet with overwrite mode:\n",
    "df.write.mode('overwrite').parquet(\"movies.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_3: write parquet with partition:\n",
    "df.write.partitionBy(\"binary\").parquet(\"movies_partitioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_1: Read parquet file:\n",
    "par_df = spark.read.parquet(\"movies.parquet\")\n",
    "len(par_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method_2: Read partitioned parquet file:\n",
    "par_df = spark.read.parquet(\"movies_partitioned.parquet/binary=FAIL\")\n",
    "len(par_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. SQL on dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count_pass|\n",
      "+----------+\n",
      "|       803|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df_table\")\n",
    "spark.sql(\"select count(*) as count_pass from df_table where binary='PASS'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+\n",
      "|summary|              budget|        budget_2013|       period_code|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "|  count|                1794|               1794|              1615|\n",
      "|   mean|4.4826462614269786E7|5.546460845150502E7| 2.419814241486068|\n",
      "| stddev| 4.818602611895356E7|5.491863559804196E7|1.1946197915091876|\n",
      "|    min|              7000.0|             8632.0|                 1|\n",
      "|    max|              4.25E8|       4.61435929E8|                 5|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe: overall info of the col:\n",
    "df.select(['budget','budget_2013','period_code']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "+------+\n",
      "|binary|\n",
      "+------+\n",
      "|  FAIL|\n",
      "|  PASS|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct:\n",
    "print(df.select(['year']).distinct().count())\n",
    "df.select(['binary']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subtract of fails:  3\n",
      "Wall time: 1.69 s\n",
      "Directly counting pass:  3\n",
      "Wall time: 306 ms\n"
     ]
    }
   ],
   "source": [
    "# using subtract: count of PASS in year 1982:\n",
    "%time print(\"Using subtract of fails: \",df[df.year=='1982'].subtract(df[df.binary=='FAIL']).count())\n",
    "# Same can be done efficiently as:\n",
    "%time print(\"Directly counting pass: \",df[df.year=='1982'][df.binary=='PASS'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for further: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "# Also: https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
